---
layout: post
title: 随机森林学习笔记
date: 2020-07-15 00:00:00
author: "王振华"
tags: 
    - 随机森林
---


## 什么是随机森林

### Bagging思想
Bagging是bootstrap aggregating。思想就是从总体样本中 随机取一部分样本 进行训练，通过 多次 这样的结果，进行投票获取平均值 作为结果输出，这就极大避免了不好的样本数据，从而提高准确度。

举个例子：假设有1000个样本，如果按以前的思维，是直接把这1000个样本拿来训练，但现在不一样，先抽取800个样本来进行训练，假如噪声点是这800个样本以外的样本点，就很有效的避开了。

### 随机森林

Random Forest 是一种基于树模型的Bagging的优化版本，一棵树的生成肯定不如多棵树，因此就有了随机森林，解决决策树泛化能力弱的问题(可以理解成三个臭皮匠顶过诸葛亮)

而同一批数据，用同样的算法只能产生一棵树，这时Bagging策略可以帮助我们产生不同的数据集。

每棵树按照如下规则生成：
1. 如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本，作为该树的训练集；
2. 如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；
3. 每棵树都尽最大程度的生长，并且没有剪枝过程。

“随机”就是指的就是这里的两个随机性。两个随机性的引入对随机森林的分类性能 至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好的抗噪能力（比如：对缺省值不敏感）。

总的来说就是随机选择样本数，随机选取特征，随机选择分类器，建立多颗这样的决策树，然后通过这投票，决定数据属于哪一类(投票机制有一票否决制、少数服从多数、加权多数)


## 随机森林分类效果的影响因素

- 森林中任意两棵树的相关性：相关性越大，错误率越大；
- 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。

减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一一个参数。


## 随机森林有什么优缺点

优点：

- 在当前的很多数据集上，相对其他算法有很大的优势，表现良好。
- 它能够处理很高维度（feature很多）的数据，并且不用做特征选择(因为特征子集是随机选择的)。
- 在训练完后，它能够给出哪些feature比较重要。
- 训练速度快，容易做成并行化方法(训练时树与树之间是相互独立的)。
- 在训练过程中，能够检测到feature间的互相影响。
- 对于不平衡的数据集来说，它可以平衡误差。
- 即使有很大一部分的特征遗失，仍可以维持准确度。

缺点：

- 随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合。
- 对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。












