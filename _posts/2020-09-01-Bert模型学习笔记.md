---
layout: post
title: Bert模型学习笔记
date: 2020-09-01 00:00:00
author: "王振华"
tags: 
    - Bert
---




## 1. 什么是 BERT
BERT 全称是 Bidirectional Encoder Representation from Transformers，是 Google 2018年提出的预训练模型，即双向 Transformer 的 Encoder，因为 Decoder 是不能获得要预测的信息的。模型的主要创新点都在pre-train方法上，即用了 Masked LM 和 Next Sentence Prediction 两种方法 分别捕捉词语和句子级别的 representation。
它有重大的理论或模型创新吗？并没有。但效果太好了，刷新了很多NLP任务的最好性能，有些任务 还被刷爆了，这才是关键。另外一点是Bert具备 广泛的通用性，绝大部分NLP任务 都可以采用类似的 两阶段模式 直接提升效果，这个第二关键。把Bert当做最近两年NLP重大进展的集大成者 更符合事实。

## 2. 从Word Embedding到Bert模型的发展

### 2.1 图像的预训练

自从深度学习火起来后，预训练过程 就是做图像或视频领域的一种比较常规的做法，而且这种做法 很有效，能明显促进应用的效果。

那图像领域 怎么做预训练呢，上图展示了这个过程，

1. 我们设计好网络结构以后，对于图像来说 一般是CNN的多层叠加网络结构，可以先用某个训练集合 比如训练集合A或训练集合B 对这个网络进行预先训练，在A任务上或B任务上学会网络参数，然后存起来 以备后用。
2. 假设我们面临第三个任务C，采取相同的网络结构，在比较浅的几层CNN结构，网络参数初始化的时候可以加载A任务或B任务 学习好的参数，其它CNN高层参数 仍然随机初始化。
3. 之后我们用C任务的训练数据 来训练网络，此时有两种做法：
   一种是浅层加载的参数 在训练C任务过程中 不动，这种方法被称为“Frozen”;
   另一种是浅层网络参数 尽管被初始化了，在C任务训练过程中 仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整 使得更适应当前的C任务。

图像或视频领域 要做预训练一般都这么做。这样做的优点是：如果手头任务C的训练集合数据量较少的话，利用预训练出来的参数 来训练任务C，加个预训练过程 也能极大加快收敛速度，所以这种预训练方式是老少皆宜的解决方案，另外疗效又好，所以在图像处理领域 很快就流行开来。

为什么预训练可行？
对于层级的CNN结构来说，不同层级的神经元 学习到了不同类型的图像特征，由底向上 特征形成层级结构，所以预训练好的网络参数，尤其是底层的网络参数 抽取出的特征 跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数 初始化新任务网络参数的原因。而高层特征 跟任务关联较大，实际可以不使用，或采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。



### 2.2 Word Embedding

#### NNLM
NLP 里做预训练 一般选择 语言模型 任务来做。

#### Word2vec

Word2vec的网络结构和NNLM基本类似，区别是：NNLM 是输入一个单词的上文，去预测这个单词。Word2vec则是输入单词的上下文，去预测中心词（以CBOW为例）。为什么会有这样的区别呢？因为NNLM的主要任务 是要学习一个解决语言模型任务的网络结构，语言模型 就是要 看到上文预测下文，而Word embedding 只是无心插柳的一个 副产品。但Word2vec的目标就是要 Word embedding，这是 主产品，所以它完全可以随性地 这么去训练网络。

2013年 最火的 用语言模型做Word Embedding的工具 是 Word2Vec，后来又出了Glove。

#### 学会了单词的WE，怎么用？

使用方法 其实和 NNLM一样，句子中每个单词以Onehot形式作为输入，然后乘以学好的 Word Embedding 矩阵Q，就直接取出单词对应的 Word Embedding 了。

上面这种模型做法 就是 18年之前 NLP领域里预训练的典型做法，Word Embedding其实对于很多下游NLP任务是有帮助的，只是帮助没有大到 闪瞎忘记戴墨镜的围观群众的双眼而已。那么新问题来了，为什么这样训练及使用Word Embedding的效果 没有期待中那么好呢？答案很简单，因为Word Embedding有问题呗。关键是Word Embedding存在什么问题？

这片在Word Embedding头上笼罩了好几年的乌云是什么？是多义词问题。我们知道，多义词 是自然语言中 经常出现的现象，也是语言灵活性和高效性的一种体现。多义词对Word Embedding来说 有什么负面影响？如上图所示，比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，是区分不开 这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子 经过word2vec，都是预测相同的单词bank，而同一个单词 占的是同一行的参数空间，这导致两种不同的上下文信息 都会编码到相同的word embedding空间里去。所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。


### 2.3 ELMO

ELMO是“Embedding from Language Models”的简称，其实这个名字 并没有反应它的 本质思想，提出ELMO的论文题目：“Deep contextualized word representation”更能体现其精髓，而精髓在哪里？在deep contextualized这个短语，一个是deep，一个是context，其中context更关键。

在此之前的Word Embedding本质上是个 静态的方式，所谓静态指的是训练好之后 每个单词的表达就固定住了，以后 使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中 混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。

ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时 多义词无法区分，不过这没关系。在我 实际使用Word Embedding的时候，单词已经具备了 特定的上下文了，这个时候我可以根据上下文单词的语义 去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个 根据当前上下文对Word Embedding动态调整 的思路。

ELMO采用了典型的 两阶段过程，第一个阶段是利用语言模型 进行预训练；第二个阶段是在做 下游任务时，从预训练网络中 提取对应单词的网络各层的Word Embedding 作为新特征 补充到下游任务中。

上图展示的是其 预训练过程，它的网络结构采用了 双层双向LSTM，目前语言模型训练的任务目标是 根据单词  的上下文去正确预测单词  ，  之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。

图中左端的前向双层LSTM代表 正方向编码器，输入的是从左到右顺序的 除了预测单词外  的上文Context-before；右端的逆向双层LSTM  代表反方向编码器，输入的是从右到左的 逆序的句子下文Context-after；每个编码器的深度 都是两层LSTM叠加。

这个网络结构 其实在NLP中是很常用的。使用这个网络结构 利用大量语料做语言模型任务 就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子  ，句子中每个单词都能得到对应的三个Embedding:

- 最底层是单词的Word Embedding；
- 往上走是第一层双向LSTM中 对应单词位置的Embedding，这层编码单词的句法信息更多一些；
- 再往上走是第二层LSTM中 对应单词位置的Embedding，这层编码单词的语义信息更多一些。

也就是说，ELMO的预训练过程 不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者 后面都有用。

上面介绍的是ELMO的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了 下游任务的使用过程，比如我们的下游任务 仍然是QA问题:

1. 此时对于问句X，我们可以 先将句子X作为预训练好的ELMO网络的输入（所谓的根据具体任务的上下文做调整，其实就是在这里做的，因为它在生成每一个词向量时，又考虑到了具体任务的上下文词语。它不是像Word2vec那样生成一个固定的词向量矩阵，使用的时候直接从矩阵中拿不再改变的词向量），这样句子X中每个单词 在ELMO网络中 都能获得对应的三个Embedding（这里是指单词特征、句法特征、语义特征）；
2. 之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重 可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个；
3. 然后 将整合后的这个Embedding作为X句 在自己任务的那个网络结构中 对应单词的输入，以此作为补充的新特征 给下游任务使用。对于上图所示 下游任务QA中的回答句子Y来说 也是如此处理。

因为ELMO给下游提供的是 每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。

前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后 多义词问题解决了吗？解决了，而且比我们期待的解决得还要好。对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词 大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中 体育领域的数量明显占优导致；而使用ELMO，根据上下文动态调整后的embedding 不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性 也是相同的，这是超出期待之处。之所以会这样，是因为我们上面提到过，第一层LSTM编码了很多句法信息，这在这里起到了重要作用。

ELMO有什么值得改进的缺点呢？

- 首先，一个非常明显的缺点在特征抽取器选择方面，ELMO 使用了LSTM 而不是新贵Transformer，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，很多研究已经证明了Transformer提取特征的能力 是要远强于LSTM的。如果ELMO采取Transformer作为特征提取器，那么估计Bert的反响远不如现在的这种火爆场面。
- 另外一点，ELMO采取双向拼接 这种融合特征的能力 可能比Bert一体化的融合特征方式弱，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。


### 2.4 GPT

GPT是“Generative Pre-Training”的简称，从名字看其含义 是指的 生成式的预训练。GPT也采用两阶段过程，第一个阶段是利用语言模型 进行预训练，第二阶段通过Fine-tuning的模式 解决下游任务。

上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同 在于两点：

- 首先，特征抽取器不是用的RNN，而是Transformer，它的特征抽取能力要强于RNN，这个选择是很明智的。Transformer 是个叠加的“自注意力机制（self attention）”构成的深度网络，是目前NLP里最强的特征提取器，注意力机制在此被发扬光大，Transformer一跃成功踢开RNN和CNN传统特征提取器，荣升头牌。Transformer在未来会逐渐替代掉RNN成为主流的NLP工具，RNN一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的。
- 其次，GPT的预训练 虽然仍然是以语言模型作为目标任务，但采用的是 单向的语言模型，所谓“单向”的含义是指：语言模型训练的任务目标是根据  单词的上文 去正确预测单词  ，  之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。ELMO在做语言模型预训练时，预测单词  同时使用了上文和下文，而GPT则只采用Context-before即这个单词的上文来进行预测，而抛开了下文。这个选择 现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候 是可以允许同时看到上文和下文一起做决策的。如果预训练时不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。

GPT训练好之后如何使用？

首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。

然后，在做下游任务时，利用第一步预训练好的参数 初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了。

再次，你可以 用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络适合解决手头的问题。

这和图像领域做预训练的过程一模一样。


### 2.5 BERT

Bert采用 和GPT完全相同的 两阶段模型，首先是语言模型预训练；其次是 使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于 在预训练阶段采用了类似ELMO的双向语言模型，即双向的Transformer，当然另外一点是语言模型的数据规模 要比GPT大。

BERT预训练模型 分为以下三个步骤：Embedding、Masked LM、Next Sentence Prediction

#### 2.5.1 Embedding

这里的Embedding由三种Embedding求和而成：
- Token Embeddings是 词向量，第一个单词是CLS标志，可以用于之后的分类任务
- Segment Embeddings每个句子都有个 句子整体的embedding，用来区别两种句子，因为预训练不光做LM，还要做以两个句子为输入的 分类任务
- Position Embeddings和之前文章中的Transformer不一样，不是三角函数 而是学习出来的

### 2.5.2 Masked LM

MLM可以理解为 完形填空，随机用【mask】掩码来代替语料中15%的词，用其上下文来做预测，

例如：my dog is hairy → my dog is [MASK]

此处将hairy进行了mask处理，然后去预测mask位置的词是什么。但是该方法有一个问题，因为是mask15%的词，其数量已经很高了。训练过程大量看到【mask】标记，这会引导模型认为输出是针对【mask】这个标记的，但后面真正用的时候是不会有这个标记的。为了解决这个问题，作者做了如下的处理：

80%是采用[mask]，my dog is hairy → my dog is [MASK]

10%是 *随机取一个词来代替mask的词，my dog is hairy -> my dog is apple

10%保持不变，my dog is hairy -> my dog is hairy

注意：这里的10%是15%需要mask中 的10%

那么为啥要以一定的概率使用随机词呢？这是因为transformer要保持对每个输入token 分布式的表征，否则Transformer很可能会记住这个[MASK]就是"hairy"。至于使用随机词带来的负面影响，文章中解释说，所有其他的token(即非"hairy"的token)共享15%*10% = 1.5%的概率，其影响是可以忽略不计的。Transformer全局的可视，又增加了 信息的获取，但是不让模型获取全量信息。


### 2.5.3 Next Sentence Prediction

选择一些句子对A与B，其中50%的数据 B是A的下一条句子，剩余50%的数据 B是语料库中随机选择的，学习其中的相关性，添加这样的预训练的目的是 目前很多NLP的任务比如QA和NLI 都需要理解两个句子之间的关系，从而能让预训练的模型 更好的适应这样的任务。
个人理解：

- Bert先是用Mask来提高视野范围的 信息获取量，增加duplicate再随机Mask，这样跟RNN类方法依次训练预测 没什么区别了 除了mask不同位置外；
- 全局视野 极大地降低了学习的难度，然后再用A+B/C来作为样本，这样每条样本都有50%的概率看到一半左右的噪声；
- 但直接学习Mask A+B/C是没法学习的，因为不知道哪些是噪声，所以又加上next_sentence预测任务，与MLM同时进行训练，这样用next来辅助模型对噪声/非噪声的辨识，用MLM来完成语义的大部分的学习。











