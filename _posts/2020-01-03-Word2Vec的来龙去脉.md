---
layout: post
title: Word2Vec的来龙去脉
date: 2020-01-03 00:00:00
author: "王振华"
tags:
    - NLP
    - Word2Vec
---



## 1. 什么是词嵌入(Word Embedding)

⾃然语⾔ 是⼀套⽤来表达含义的 复杂系统，其中词是 表义的基本单元。

在NLP(自然语言处理)领域，文本表示是第一步，通俗来说就是 把人类的语言符号 转化为机器能够进行计算的数字，因为普通的文本语言 机器是看不懂的，必须通过转化来表征对应文本。早期是基于规则的方法进行转化，而现代的方法是基于统计机器学习的方法。

数据 决定了机器学习的上限，而算法 只是尽可能逼近这个上限，在本文中 数据指的就是文本表示，所以，弄懂文本表示的发展历程，对于NLP学习者来说 是必不可少的。

顾名思义，词向量是⽤来表⽰词 的向量，也可被认为是词的 特征向量或表征。把词 映射为实数域向量的技术 也叫词嵌⼊（word embedding）。近年来，词嵌⼊已逐渐成为⾃然语⾔处理的 基础知识。


文本表示分为离散表示和分布式表示：




## 2.离散表示

### 2.1 One-hot表示

One-hot简称 独热向量编码，也是特征工程中 最常用的方法。其步骤如下：

1. 构造文本分词后的字典，每个分词是一个比特值，比特值为 0或1。
2. 每个分词的文本表示为 该分词的比特位为1，其余位为0 的矩阵。

例如：

John likes to watch movies. Mary likes too

John also likes to watch football games.

以上两句可以构造一个词典：

{"John": 1, "likes": 2, "to": 3, "watch": 4, "movies": 5, "also": 6, "football": 7, "games": 8, "Mary": 9, "too": 10} 

每个词典索引对应着比特位。那么利用One-hot表示为：

John: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] 

likes: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] .......等等，以此类推。

One-hot表示文本信息的缺点：
- 随着语料库的增加，数据特征的维度 会越来越大，产生一个维度很高，又很稀疏的矩阵。
- 这种表示方法的分词顺序和在句子中的顺序 是无关的，不能保留词与词之间的关系信息。


### 2.2 词袋模型

词袋模型(Bag-of-words model)，像句子或文件这样的文字 可以用一个袋子装着这些词的方式 表现，这种表现方式 不考虑文法以及词的顺序。

文档的向量表示 可以直接将各词的词向量表示 加和。例如：

John likes to watch movies. Mary likes too. 

John also likes to watch football games.

以上两句可以构造一个词典，{"John": 1, "likes": 2, "to": 3, "watch": 4, "movies": 5, "also": 6, "football": 7, "games": 8, "Mary": 9, "too": 10} 

那么第一句的向量表示为：[1,2,1,1,1,0,0,0,1,1]，其中的2表示likes在该句中出现了2次，依次类推。

词袋模型同样有以下缺点：
- 词向量化后，词与词之间 是有大小关系的，不一定词出现的越多，权重越大。
- 词与词之间是没有顺序关系的。



### 2.3 TF-IDF

TF-IDF（term frequency–inverse document frequency）是一种用于 信息检索与数据挖掘的 常用加权技术。TF意思是词频(Term Frequency)，IDF意思是 逆文本频率指数(Inverse Document Frequency)。

字词的重要性 随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率 成反比下降。一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。

![TF-IDF公式](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-21_10-7-23.png)

分母之所以加1，是为了避免分母为0。

那么，，从这个公式可以看出，当一个词w在文档中出现的次数增大时，TF-IDF的值是减小的。

缺点：还是没有把 词与词之间的 顺序关系 表达出来。


### 2.4 n-gram模型

n-gram模型为了保持 词的顺序，做了一个滑窗的操作，这里的n表示的就是滑窗的大小，例如2-gram模型，也就是把2个词当做一组来处理，然后向后移动一个词的长度，再次组成另一组词，把这些生成一个字典，按照词袋模型的方式 进行编码得到结果。

例如：

John likes to watch movies. Mary likes too

John also likes to watch football games

以上两句可以构造一个词典，{"John likes”: 1, "likes to”: 2, "to watch”: 3, "watch movies”: 4, "Mary likes”: 5, "likes too”: 6, "John also”: 7, "also likes”: 8, “watch football”: 9, "football games": 10}

那么第一句的向量表示为：[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]，其中第一个1表示John likes在该句中出现了1次，依次类推。

缺点：随着n的大小增加，词表 会成指数型膨胀。



### 2.5 离散表示 存在的问题

- 无法衡量 词向量之间的关系。
- 词表的维度 随着语料库的增长而膨胀。
- n-gram词序列 随语料库增长呈指数型膨胀。
- 离散数据来表示文本 会带来数据稀疏问题，导致丢失了信息，与我们生活中理解的信息 是不一样的。

由于存在以上问题，对于一般的NLP问题，是可以使用离散表示文本信息来解决的，但对于精度要求较高的场景 就不适合了。





## 3. 分布式表示

科学家们为了提高模型的精度，又发明出了分布式的表示文本信息的方法。

用一个词附近的其它词 来表示该词，这是现代 统计自然语言处理中 最有创见的想法之一。当初科学家发明这种方法 是基于人的语言表达，认为一个词是由这个词的周边词汇一起 来构成精确的语义信息。就好比，物以类聚 人以群分，如果你想了解一个人，可以通过他周围的人进行了解。


### 3.1 共现矩阵

共现矩阵顾名思义就是 共同出现的意思，词文档的共现矩阵 主要用于 发现主题(topic)，用于主题模型，如LSA。

局域窗中的word-word共现矩阵 可以挖掘语法和语义信息，例如：

- I like deep learning.	
- I like NLP.	
- I enjoy flying

以上三句话，设置滑窗为2，可以得到一个词典：{"I like","like deep","deep learning","like NLP","I enjoy","enjoy flying"}。

我们可以得到一个共现矩阵(对称矩阵)：
![image](https://wx2.sinaimg.cn/large/00630Defly1g2rwv1op5zj30q70c7wh2.jpg)

中间的每个格子表示的是 行和列组成的词组 在词典中共同出现的次数，也就体现了共现的特性。

存在的问题：

- 向量维数随着词典大小 线性增长。
- 存储整个词典的 空间消耗非常大。
- 一些模型如文本分类模型 会面临稀疏性问题。
- 模型会欠稳定，每新增一份语料进来，稳定性就会变化。




## 4.神经网络表示

### 4.1 NNLM

NNLM (Neural Network Language model)，神经网络语言模型 是03年提出来的，通过训练得到 中间产物--词向量矩阵，这就是我们要得到的 文本表示向量矩阵。

NNLM说的是定义一个窗口大小，把 这个窗口中 最后一个词当做y，把之前的词 当做输入x，通俗来说就是 预测这个窗口中最后一个词 出现概率的 模型。

![NNLM窗口](https://wx3.sinaimg.cn/large/00630Defly1g2vb5thw9rj30eq065dg4.jpg)

以下是NNLM的网络结构图：
![NNLM](https://wx3.sinaimg.cn/large/00630Defly1g2t1f4bqilj30lv0e2adl.jpg)


- input层是一个 前向词的输入，是经过one-hot编码的词向量表示形式，具有V*1的矩阵。
- C矩阵是 投影矩阵，也就是 稠密词向量表示，在神经网络中是w参数矩阵，该矩阵的大小为D*V，正好与input层进行全连接(相乘)得到D*1的矩阵，采用线性映射 将one-hot表示 投影到 稠密D维表示。
  ![投影矩阵](https://wx3.sinaimg.cn/large/00630Defly1g2t1s20jpnj30f107575i.jpg)
  
  one-hot 编码 * 词向量矩阵 得到的是 输入词的 词向量， 该词向量 * 上下文矩阵，得到的是输入词 与 词库中各个词的关系/相似度/是否是上下文等 的一个向量，其中的每个元素表示 一个词。这个向量经过softmax就得到了 是各个词的概率
- output层(softmax)自然是 前向窗中 需要预测的词。
- 通过BP＋SGD得到最优的C投影矩阵，这就是NNLM的中间产物，也是我们所求的文本表示矩阵，通过NNLM将稀疏矩阵 投影到稠密向量矩阵中。


### 4.2 Word2Vec

谷歌2013年提出的Word2Vec是目前最常用的词嵌入模型之一。Word2Vec实际是一种 浅层的神经网络模型，和NNLM很类似，但比NNLM简单。

它有两种网络结构，分别是CBOW（Continous Bag of Words）连续词袋和Skip-gram。

CBOW获得中间词两边的的上下文，然后 用周围的词 去预测中间的词，把中间词当做y，把窗口中的其它词 当做x输入，x输入是经过one-hot编码过的，然后通过一个隐层进行求和操作，最后通过激活函数softmax，可以计算出 每个单词的生成概率，接下来的任务就是训练神经网络的权重，使得语料库中 所有单词的整体生成概率 最大化，而求得的权重矩阵 就是文本表示词向量的结果。

Skip-gram是通过当前词 来预测窗口中上下文词 出现的概率模型，把当前词当做x，把窗口中其它词当做y，依然是通过一个隐层接一个Softmax激活函数 来预测其它词的概率。如下图所示：

![skip-gram](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-20_20-34-0.jpg)




## 5.Word2Vec存在的问题

- 对每个local context window单独训练，没有利用包含在 global co-currence矩阵中的 统计信息。
- 对多义词无法很好的表示和处理，因为使用了 唯一的词向量

- 没考虑词序
- 对于中文，依赖 分词结果的好坏
- 对新生词 无法友好处理





参考：
- https://cloud.tencent.com/developer/article/1591734
- http://captainbed.top/5-2-5/