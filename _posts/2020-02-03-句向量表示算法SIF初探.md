---
layout: post
title: 句向量表示算法SIF初探
date: 2020-02-03 00:00:00
author: "王振华"
tags: 
    - SIF
    - 句向量
---

既然词可以embedding，句子应该也可以。其实，万物皆可embedding。



## 基于词向量的词袋模型

获取sentence embedding 最直接最简单的思路就是对一个句子中所有词的 word embedding 进行组合。这种方法最明显的缺点是没有考虑次序信息，但足够简单高效，在一些任务上是很好的 baseline。

### 平均词向量与TFIDF加权平均词向量
平均词向量就是将句子中所有词的word embedding 相加取平均，得到的向量就当做最终的 sentence embedding。这种方法的缺点是认为句子中的所有词对于表达句子含义同样重要。TFIDF加权平均词向量就是对每个词按照 tfidf 进行打分，然后进行加权平均，得到最终的句子表示。

### SIF加权平均词向量

发表于2016年的论文《A simple but tough-to-beat baseline for sentence embeddings》提出了一种非常简单但是具有一定竞争力的句子向量表示算法。算法包括两步，第一步是对句子中所有的词向量进行加权平均，得到平均向量；第二步是移出在所有句子向量组成的矩阵的第一个主成分上的投影，因此该算法被简记为WR。
第一步主要是对TFIDF加权平均词向量表示句子的方法进行改进。论文提出了一种平滑倒词频 (smooth inverse frequency, SIF)方法用于计算每个词的加权系数，具体地，词的权重为 (a / (a+p(w)))，其中 a 为平滑参数，p(w)为（估计的）词频。直观理解SIF，就是说频率越低的词在当前句子出现了，说明它在句子中的重要性更大，也就是加权系数更大。事实上，如果把一个句子认为是一篇文档并且假设该句中不出现重复的词（TF=1），那么TFIDF将演变成IF，即未平滑的倒词频。但是相较于TFIDF这种经验式公式，论文通过理论证明为SIF提供理论依据。对于第二步，是移出所有句子的共有信息，因此保留下来的句子向量更能够表示本身并与其它句子向量产生差距。




参考：https://cloud.tencent.com/developer/article/1432882